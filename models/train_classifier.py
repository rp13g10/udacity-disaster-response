import nltk
import numpy as np
import os
import pandas as pd
import sqlalchemy as sql
import string
import sys

from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.pipeline import Pipeline

def load_data(db_path):

    db_path = os.path.realpath(db_path)
    db_conn = sql.create_engine(f'sqlite:///{db_path}')

    messages = pd.read_sql_table('messages', db_conn, index_col='id')
    categories = pd.read_sql_table('categories', db_conn, index_col='id')

    matches = sum(messages.index == categories.index)
    msg_count = len(messages.index)
    cat_count = len(categories.index)

    # Check that record IDs are aligned between datasets
    assert matches == msg_count == cat_count, "Error: datasets are not aligned."

    X = messages['message'].to_numpy()
    y = categories.to_numpy()
    cat_names = categories.columns.tolist()

    return X, y, cat_names


class Tokenizer(BaseEstimator, TransformerMixin):

    def __init__(self):
        self.wnl = WordNetLemmatizer()
        self.stop = stopwords.words('english')

    def _get_wordnet_pos(self, treebank_tag):
        '''Turns a treebank tag (generated by nltk) into a wordnet tag which
        can be used by the lemmatzier'''

        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            # Default parameter
            return wordnet.NOUN

    def _tokenize(self, comment):
        comment = comment.lower()
        tokens = nltk.word_tokenize(comment)
        tokens = [x for x in tokens
                  if x
                  and x not in string.punctuation]

        try:
            pos_tags = nltk.pos_tag(tokens)
        except LookupError:
            nltk.download('averaged_perceptron_tagger')
            pos_tags = nltk.pos_tag(tokens)

        assert len(tokens) == len(pos_tags), 'Error: Mismatched tags and tokens'

        tokens = [self.wnl.lemmatize(token,
                                     pos=self._get_wordnet_pos(pos_tag))
                  for token, pos_tag
                  in pos_tags
                  if token not in self.stop]

        return tokens

    def __call__(self, comment):
        return self._tokenize(comment)


X, y, cat_names = load_data(
    r"C:\Users\ross-\Documents\GitHub\udacity-disaster-response\data\udacity.db")
        
pipe = Pipeline([
        ('features', CountVectorizer(tokenizer=Tokenizer())),
        ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))
])

pipe = pipe.fit(X, y)





# def build_model():
    
#     pipe = Pipeline([
#         ('features', Tokenizer),
#         ('clf', RandomForestClassifier(random_state=42))
#     ])


# def evaluate_model(model, X_test, Y_test, category_names):
#     pass


# def save_model(model, model_filepath):
#     pass


# def main():
#     if len(sys.argv) == 3:
#         db_path, model_filepath = sys.argv[1:]
#         # print('Loading data...\n    DATABASE: {}'.format(db_path))
#         X, Y, category_names = load_data(db_path)
#         X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)
        
#         print('Building model...')
#         model = build_model()
        
#         print('Training model...')
#         model.fit(X_train, Y_train)
        
#         print('Evaluating model...')
#         evaluate_model(model, X_test, Y_test, category_names)

#         print('Saving model...\n    MODEL: {}'.format(model_filepath))
#         save_model(model, model_filepath)

#         print('Trained model saved!')

#     else:
#         print('Please provide the filepath of the disaster messages database '\
#               'as the first argument and the filepath of the pickle file to '\
#               'save the model to as the second argument. \n\nExample: python '\
#               'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


# if __name__ == '__main__':
#     main()